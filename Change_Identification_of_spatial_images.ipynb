{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf+obGJxECahtf8O3lWR4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjonnill07/AI-ML-experiment-Notebooks/blob/main/Change_Identification_of_spatial_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title 2. Mount Google Drive & Set Dataset Path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MblCpmGaRvzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "LEVIR-CD Change Detection (Optimized for Colab Pro)\n",
        "\n",
        "Trains a more robust Siamese UNet++ like model with a powerful encoder\n",
        "at higher resolution for potentially better change detection accuracy.\n",
        "Visualizes changes using bounding boxes.\n",
        "\"\"\"\n",
        "\n",
        "# @title 1. Setup: Install Libraries, Import Modules, Check GPU\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Adapt cuXXX if needed\n",
        "!pip install -q segmentation-models-pytorch albumentations opencv-python-headless matplotlib torchinfo\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm # Progress bar\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchinfo import summary # For model inspection\n",
        "\n",
        "# For reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "# torch.backends.cudnn.deterministic = True # Can slow down training slightly\n",
        "# torch.backends.cudnn.benchmark = True # Set True if input sizes don't vary, usually faster\n",
        "\n",
        "# Check GPU and Colab environment\n",
        "print(\"--- System Information ---\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_info = !nvidia-smi --query-gpu=gpu_name,memory.total --format=csv,noheader\n",
        "    print(f\"GPU Detected: {gpu_info[0]}\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Training will be very slow. Ensure GPU is enabled in Runtime settings.\")\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "# Simple check for Colab Pro (might not be 100% reliable)\n",
        "# Pro often gives V100, A100, or P100 GPUs with more RAM\n",
        "if 'google.colab' in sys.modules:\n",
        "     if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory / (1024**3) > 16: # Check if > 16GB GPU RAM\n",
        "         print(\"High-RAM GPU detected, likely Colab Pro environment.\")\n",
        "     else:\n",
        "         print(\"Standard Colab GPU or CPU detected. Performance might be limited.\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(\"------------------------\")"
      ],
      "metadata": {
        "id": "BrMlK77cRm7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArmFKE8DRbwp"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- !!! IMPORTANT: SET YOUR DATASET PATH !!! ---\n",
        "# Adjust this path to where you unzipped the LEVIR-CD dataset folder in your Google Drive\n",
        "# It should contain subfolders like 'train', 'val', 'test'\n",
        "# Inside each, there should be 'A', 'B', 'label' subfolders.\n",
        "DRIVE_DATASET_PATH = \"/content/drive/MyDrive/LEVIR-CD+\" # <--- CHANGE THIS\n",
        "\n",
        "if not os.path.exists(DRIVE_DATASET_PATH):\n",
        "    print(f\"ERROR: Dataset path not found: {DRIVE_DATASET_PATH}\")\n",
        "    print(\"Please ensure you have uploaded the dataset to Google Drive and updated the path.\")\n",
        "    sys.exit() # Stop execution if path is invalid\n",
        "else:\n",
        "    print(f\"Dataset path confirmed: {DRIVE_DATASET_PATH}\")\n",
        "    # Optional: List contents to verify\n",
        "    # !ls -l $DRIVE_DATASET_PATH\n",
        "\n",
        "# @title 3. Configuration (Enhanced for Pro)\n",
        "\n",
        "# --- Model & Training Hyperparameters ---\n",
        "IMG_SIZE = 512 # Increased resolution for better detail\n",
        "# Adjust Batch size based on your specific Colab Pro GPU (V100/A100 can likely handle more)\n",
        "# Start with 16 for 512x512, decrease to 8 or 12 if you get OOM errors.\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4 # AdamW default LR is often a good starting point\n",
        "NUM_EPOCHS = 50 # Increased epochs for potentially better convergence\n",
        "# Choose a more powerful encoder\n",
        "# Options: 'efficientnet-b4', 'efficientnet-b5', 'resnext50_32x4d', 'timm-regnety_032' etc.\n",
        "ENCODER_NAME = 'efficientnet-b4'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "# Use Unet++ which can sometimes capture finer details\n",
        "MODEL_ARCHITECTURE = smp.UnetPlusPlus\n",
        "MODEL_SAVE_PATH = f'/content/best_cd_model_{ENCODER_NAME}_{IMG_SIZE}.pth' # Include info in filename\n",
        "PATIENCE = 7 # For ReduceLROnPlateau scheduler and early stopping\n",
        "\n",
        "# Dataset paths (derived from DRIVE_DATASET_PATH)\n",
        "TRAIN_IMG_T1_DIR = os.path.join(DRIVE_DATASET_PATH, 'train/A')\n",
        "TRAIN_IMG_T2_DIR = os.path.join(DRIVE_DATASET_PATH, 'train/B')\n",
        "TRAIN_MASK_DIR = os.path.join(DRIVE_DATASET_PATH, 'train/label')\n",
        "\n",
        "VAL_IMG_T1_DIR = os.path.join(DRIVE_DATASET_PATH, 'val/A')\n",
        "VAL_IMG_T2_DIR = os.path.join(DRIVE_DATASET_PATH, 'val/B')\n",
        "VAL_MASK_DIR = os.path.join(DRIVE_DATASET_PATH, 'val/label')\n",
        "\n",
        "# @title 4. Dataset and DataLoader (with More Augmentations)\n",
        "\n",
        "class LevirCDDataset(Dataset):\n",
        "    def __init__(self, image_dir_t1, image_dir_t2, mask_dir, transform=None):\n",
        "        self.image_dir_t1 = image_dir_t1\n",
        "        self.image_dir_t2 = image_dir_t2\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted([f for f in os.listdir(image_dir_t1) if f.endswith('.png')]) # LEVIR-CD uses .png\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path_t1 = os.path.join(self.image_dir_t1, img_name)\n",
        "        img_path_t2 = os.path.join(self.image_dir_t2, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, img_name)\n",
        "\n",
        "        # Load images\n",
        "        image_t1 = cv2.imread(img_path_t1)\n",
        "        image_t1 = cv2.cvtColor(image_t1, cv2.COLOR_BGR2RGB)\n",
        "        image_t2 = cv2.imread(img_path_t2)\n",
        "        image_t2 = cv2.cvtColor(image_t2, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if mask is None:\n",
        "             raise FileNotFoundError(f\"Mask not found or invalid: {mask_path}\")\n",
        "\n",
        "        mask = mask / 255.0 # Normalize 0-1\n",
        "        mask = mask.astype(np.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image_t1, image1=image_t2, mask=mask)\n",
        "            image_t1 = augmented['image']\n",
        "            image_t2 = augmented['image1']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        mask = np.expand_dims(mask, axis=0) # Add channel dim: (1, H, W)\n",
        "\n",
        "        return image_t1, image_t2, mask\n",
        "\n",
        "# Define Transforms using Albumentations\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "# Enhanced Augmentations for Training\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE, interpolation=cv2.INTER_LINEAR), # Specify interpolation\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    # Add more complex geometric transforms\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5,\n",
        "                       border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "    # Add color/brightness augmentations (applied independently to T1/T2 after geometric)\n",
        "    # Use OneOf to apply only one type of color augmentation sometimes\n",
        "    A.OneOf([\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "    ], p=0.7), # Apply one of these 70% of the time\n",
        "\n",
        "    # Normalization should come after color augmentations but before ToTensor\n",
        "    A.Normalize(mean=mean, std=std, max_pixel_value=255.0), # Normalize RGB images\n",
        "    ToTensorV2() # Converts numpy HWC [0,1] to torch CHW tensor\n",
        "], additional_targets={'image1': 'image'}) # Ensures geometric transforms are identical\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE, interpolation=cv2.INTER_LINEAR),\n",
        "    A.Normalize(mean=mean, std=std, max_pixel_value=255.0),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'image1': 'image'})\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = LevirCDDataset(TRAIN_IMG_T1_DIR, TRAIN_IMG_T2_DIR, TRAIN_MASK_DIR, transform=train_transform)\n",
        "val_dataset = LevirCDDataset(VAL_IMG_T1_DIR, VAL_IMG_T2_DIR, VAL_MASK_DIR, transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "# num_workers=4 might be feasible on Pro, adjust if needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f\"--- Dataset Info ---\")\n",
        "print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
        "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "print(f\"--------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Model Architecture (Siamese UNet++ Concatenation)\n",
        "\n",
        "class SiamUnetPlusPlusConcatenate(nn.Module):\n",
        "    \"\"\"\n",
        "    Siamese UNet++ where T1 and T2 are concatenated along the channel\n",
        "    dimension and fed into a UNet++ whose first layer is modified.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_name='efficientnet-b4', encoder_weights='imagenet', classes=1, activation=None):\n",
        "        super().__init__()\n",
        "        self.model = MODEL_ARCHITECTURE( # Using the configured architecture (e.g., UnetPlusPlus)\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=3, # Start with 3, modify below\n",
        "            classes=classes,\n",
        "            activation=activation,\n",
        "        )\n",
        "\n",
        "        # --- Modify the first convolution layer to accept 6 channels ---\n",
        "        # Layer name depends on the encoder in SMP. Inspect or check SMP docs.\n",
        "        # For ResNets: self.model.encoder.conv1\n",
        "        # For EfficientNets: self.model.encoder._conv_stem\n",
        "        layer_to_modify = None\n",
        "        original_weights = None\n",
        "        try:\n",
        "            if hasattr(self.model.encoder, '_conv_stem'): # EfficientNet style\n",
        "                 layer_to_modify = self.model.encoder._conv_stem\n",
        "                 print(\"Identified EfficientNet style stem layer.\")\n",
        "            elif hasattr(self.model.encoder, 'conv1'): # ResNet style\n",
        "                 layer_to_modify = self.model.encoder.conv1\n",
        "                 print(\"Identified ResNet style conv1 layer.\")\n",
        "            else:\n",
        "                 raise AttributeError(\"Could not find standard first conv layer ('_conv_stem' or 'conv1') in the encoder.\")\n",
        "\n",
        "            original_weights = layer_to_modify.weight.clone()\n",
        "            new_conv = nn.Conv2d(\n",
        "                6, # New input channels\n",
        "                layer_to_modify.out_channels,\n",
        "                kernel_size=layer_to_modify.kernel_size,\n",
        "                stride=layer_to_modify.stride,\n",
        "                padding=layer_to_modify.padding,\n",
        "                bias=layer_to_modify.bias is not None\n",
        "            )\n",
        "\n",
        "            # Initialize weights: copy first 3 channels, initialize next 3 (e.g., copy or Kaiming)\n",
        "            new_conv.weight.data[:, :3, :, :] = original_weights\n",
        "            # Simple initialization: Copy weights for the second set of channels\n",
        "            new_conv.weight.data[:, 3:, :, :] = original_weights\n",
        "            # # Alternative: Kaiming initialization for the second set\n",
        "            # torch.nn.init.kaiming_normal_(new_conv.weight.data[:, 3:, :, :], mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "            # Replace the layer in the model\n",
        "            if hasattr(self.model.encoder, '_conv_stem'):\n",
        "                 self.model.encoder._conv_stem = new_conv\n",
        "                 print(f\"Replaced encoder._conv_stem with 6-channel input. New shape: {new_conv.weight.shape}\")\n",
        "            elif hasattr(self.model.encoder, 'conv1'):\n",
        "                 self.model.encoder.conv1 = new_conv\n",
        "                 print(f\"Replaced encoder.conv1 with 6-channel input. New shape: {new_conv.weight.shape}\")\n",
        "\n",
        "        except AttributeError as e:\n",
        "             print(f\"Error modifying first conv layer: {e}\")\n",
        "             print(\"Model inspection might be needed.\")\n",
        "             # You might need to print(self.model.encoder) to find the exact layer name\n",
        "             sys.exit()\n",
        "        except Exception as e:\n",
        "             print(f\"An unexpected error occurred during layer modification: {e}\")\n",
        "             sys.exit()\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x1, x2], dim=1) # Shape: (Batch, 6, H, W)\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "# Instantiate the model\n",
        "model = SiamUnetPlusPlusConcatenate(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    classes=1,\n",
        "    activation=None # Output logits for BCEWithLogitsLoss\n",
        ").to(DEVICE)\n",
        "\n",
        "# Inspect model structure and parameter count (optional)\n",
        "print(\"\\n--- Model Summary ---\")\n",
        "try:\n",
        "    # Provide example input tensor shapes (Batch, Channels, Height, Width)\n",
        "    summary(model, input_size=[(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE), (BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)], device=str(DEVICE))\n",
        "except Exception as e:\n",
        "    print(f\"Could not generate model summary: {e}\")\n",
        "print(\"---------------------\\n\")\n",
        "\n",
        "# @title 6. Loss Function, Optimizer, Metrics, Scheduler\n",
        "\n",
        "# Loss: Combination of BCE and Dice\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=True)\n",
        "\n",
        "def combined_loss(logits, targets, bce_weight=0.6, dice_weight=0.4):\n",
        "    bce = bce_loss(logits, targets)\n",
        "    dice = dice_loss(logits, targets)\n",
        "    return bce * bce_weight + dice * dice_weight\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5) # Added small weight decay\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=PATIENCE // 2, verbose=True) # Reduce LR based on validation IoU\n",
        "\n",
        "# Metrics - IoU and F1 Score (common for segmentation)\n",
        "def calculate_metrics(logits, targets, threshold=0.5, smooth=1e-6):\n",
        "    with torch.no_grad():\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > threshold).float()\n",
        "\n",
        "        # Flatten spatial dimensions (Batch, 1, H, W) -> (Batch, H*W)\n",
        "        preds_flat = preds.view(preds.size(0), -1)\n",
        "        targets_flat = targets.view(targets.size(0), -1)\n",
        "\n",
        "        # True Positives, False Positives, False Negatives\n",
        "        tp = (preds_flat * targets_flat).sum(dim=1)\n",
        "        fp = (preds_flat * (1 - targets_flat)).sum(dim=1)\n",
        "        fn = ((1 - preds_flat) * targets_flat).sum(dim=1)\n",
        "\n",
        "        # IoU (Jaccard)\n",
        "        iou = (tp + smooth) / (tp + fp + fn + smooth)\n",
        "\n",
        "        # F1 Score\n",
        "        f1 = (2 * tp + smooth) / (2 * tp + fp + fn + smooth)\n",
        "\n",
        "        return iou.mean(), f1.mean() # Average over batch\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f0istcNzSPGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Training Loop with Metrics and Early Stopping\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(loader, desc=f\"Training Epoch {epoch+1}/{NUM_EPOCHS}\", leave=True, dynamic_ncols=True)\n",
        "    for batch_idx, (img_t1, img_t2, mask) in enumerate(pbar):\n",
        "        img_t1, img_t2, mask = img_t1.to(device), img_t2.to(device), mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(img_t1, img_t2)\n",
        "        loss = combined_loss(outputs, mask)\n",
        "        loss.backward()\n",
        "        # Optional: Gradient clipping can sometimes help stabilize training\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    return avg_loss\n",
        "\n",
        "def validate_one_epoch(model, loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_iou = 0.0\n",
        "    running_f1 = 0.0\n",
        "    pbar = tqdm(loader, desc=f\"Validating Epoch {epoch+1}/{NUM_EPOCHS}\", leave=True, dynamic_ncols=True)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img_t1, img_t2, mask) in enumerate(pbar):\n",
        "            img_t1, img_t2, mask = img_t1.to(device), img_t2.to(device), mask.to(device)\n",
        "\n",
        "            outputs = model(img_t1, img_t2)\n",
        "            loss = combined_loss(outputs, mask)\n",
        "            iou, f1 = calculate_metrics(outputs, mask)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_iou += iou.item()\n",
        "            running_f1 += f1.item()\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", iou=f\"{iou.item():.4f}\", f1=f\"{f1.item():.4f}\")\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    avg_iou = running_iou / len(loader)\n",
        "    avg_f1 = running_f1 / len(loader)\n",
        "    return avg_loss, avg_iou, avg_f1\n",
        "\n",
        "# --- Training Initialization ---\n",
        "best_val_iou = 0.0\n",
        "epochs_no_improve = 0 # For early stopping\n",
        "train_losses, val_losses, val_ious, val_f1s = [], [], [], []\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"🚀 Starting Training...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, DEVICE)\n",
        "    val_loss, val_iou, val_f1 = validate_one_epoch(model, val_loader, DEVICE)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_ious.append(val_iou)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start_time\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss  : {val_loss:.4f}\")\n",
        "    print(f\"  Val IoU   : {val_iou:.4f}\")\n",
        "    print(f\"  Val F1    : {val_f1:.4f}\")\n",
        "    print(f\"  LR        : {current_lr:.6f}\")\n",
        "    print(f\"  Duration  : {epoch_duration:.2f}s\")\n",
        "\n",
        "\n",
        "    # Learning rate scheduling based on validation IoU (higher is better)\n",
        "    scheduler.step(val_iou)\n",
        "\n",
        "    # Save the model if validation IoU improves\n",
        "    if val_iou > best_val_iou:\n",
        "        print(f\"✅ Validation IoU improved ({best_val_iou:.4f} --> {val_iou:.4f}). Saving model...\")\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        best_val_iou = val_iou\n",
        "        epochs_no_improve = 0 # Reset counter\n",
        "        print(f\"   Model saved to {MODEL_SAVE_PATH}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"📉 Validation IoU did not improve for {epochs_no_improve} epoch(s). Best IoU: {best_val_iou:.4f}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(f\"\\n🚫 Early stopping triggered after {PATIENCE} epochs without improvement.\")\n",
        "        break\n",
        "\n",
        "total_training_time = time.time() - start_time\n",
        "print(\"\\n🏁 Training Finished!\")\n",
        "print(f\"Best Validation IoU: {best_val_iou:.4f}\")\n",
        "print(f\"Total Training Time: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
        "\n",
        "# @title 8. Plot Training History\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(range(1, len(val_ious) + 1), val_ious, label='Validation IoU', color='green')\n",
        "plt.title('Validation IoU History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('IoU')\n",
        "plt.axhline(y=best_val_iou, color='r', linestyle='--', label=f'Best IoU: {best_val_iou:.4f}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(range(1, len(val_f1s) + 1), val_f1s, label='Validation F1-Score', color='orange')\n",
        "plt.title('Validation F1-Score History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5zTUTni8ShhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. Bounding Box Generation Functions (Unchanged)\n",
        "\n",
        "def mask_to_bboxes(mask_np, min_area_threshold=50): # Increased default threshold slightly for larger images\n",
        "    \"\"\"\n",
        "    Converts a binary mask (numpy array 0 or 1) to bounding boxes.\n",
        "    Args:\n",
        "        mask_np: Binary mask (H, W) as a numpy array (dtype should be integer 0 or 1, or float 0.0 or 1.0).\n",
        "        min_area_threshold: Minimum contour area to be considered a valid bounding box.\n",
        "    Returns:\n",
        "        List of bounding boxes [(x, y, w, h), ...].\n",
        "    \"\"\"\n",
        "    if mask_np.ndim != 2:\n",
        "        raise ValueError(f\"Input mask must be 2D (H, W). Got shape {mask_np.shape}\")\n",
        "\n",
        "    if mask_np.dtype == np.float32 or mask_np.dtype == np.float64:\n",
        "        mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
        "    elif mask_np.dtype == np.uint8:\n",
        "         mask_uint8 = np.where(mask_np > 0, 255, 0).astype(np.uint8)\n",
        "    else:\n",
        "        mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
        "\n",
        "    contours, hierarchy = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    bboxes = []\n",
        "    for contour in contours:\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area > min_area_threshold:\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "            bboxes.append((x, y, w, h))\n",
        "\n",
        "    return bboxes\n",
        "\n",
        "def draw_bboxes_on_image(image_np_rgb, bboxes, color=(255, 0, 0), thickness=2):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes on an image (expects RGB numpy array HWC).\n",
        "    \"\"\"\n",
        "    output_image = image_np_rgb.copy()\n",
        "    if output_image.ndim != 3 or output_image.shape[2] != 3:\n",
        "         raise ValueError(f\"Input image must be 3D RGB (H, W, 3). Got shape {output_image.shape}\")\n",
        "\n",
        "    # Ensure image is uint8 for drawing\n",
        "    if output_image.dtype != np.uint8:\n",
        "        print(\"Warning: Converting input image to uint8 for drawing.\")\n",
        "        output_image = output_image.astype(np.uint8)\n",
        "\n",
        "\n",
        "    color_bgr = tuple(reversed(color)) # OpenCV uses BGR\n",
        "    img_bgr = cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)\n",
        "    for (x, y, w, h) in bboxes:\n",
        "        cv2.rectangle(img_bgr, (x, y), (x + w, y + h), color_bgr, thickness)\n",
        "    output_image_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    return output_image_rgb\n",
        "\n",
        "\n",
        "# @title 10. Inference and Visualization (using Best Model)\n",
        "\n",
        "# Load the best saved model weights\n",
        "print(f\"Loading best model from: {MODEL_SAVE_PATH}\")\n",
        "# Re-initialize model architecture (must match the trained architecture)\n",
        "inference_model = SiamUnetPlusPlusConcatenate(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=None, # Weights are loaded below\n",
        "    classes=1,\n",
        "    activation=None\n",
        ")\n",
        "\n",
        "# Load the saved state dictionary\n",
        "try:\n",
        "    # Ensure loading to the correct device\n",
        "    inference_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    inference_model.to(DEVICE) # Move model to device\n",
        "    print(\"Model weights loaded successfully to\", DEVICE)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model weights: {e}\")\n",
        "    print(f\"Ensure the path '{MODEL_SAVE_PATH}' is correct and the file exists.\")\n",
        "    sys.exit()\n",
        "\n",
        "inference_model.eval() # Set model to evaluation mode\n",
        "\n",
        "# --- Get a sample from the validation set for inference ---\n",
        "# We need a dataset *without* augmentations (except resize) for clean visualization\n",
        "vis_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE, interpolation=cv2.INTER_LINEAR)\n",
        "])\n",
        "vis_dataset = LevirCDDataset(VAL_IMG_T1_DIR, VAL_IMG_T2_DIR, VAL_MASK_DIR, transform=vis_transform)\n",
        "# Load one sample (no batching needed for single inference)\n",
        "vis_loader = DataLoader(vis_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "try:\n",
        "    img_t1_vis_np, img_t2_vis_np, mask_vis_np = next(iter(vis_loader))\n",
        "except StopIteration:\n",
        "     print(\"Validation loader is empty. Cannot perform inference visualization.\")\n",
        "     sys.exit()\n",
        "\n",
        "# Remove batch dimension and ensure correct numpy format (H, W, C for images, H, W for mask)\n",
        "img_t1_vis_np = img_t1_vis_np.squeeze(0).numpy().astype(np.uint8)\n",
        "img_t2_vis_np = img_t2_vis_np.squeeze(0).numpy().astype(np.uint8)\n",
        "mask_vis_np = mask_vis_np.squeeze(0).squeeze(0).numpy() # Remove batch and channel dims\n",
        "\n",
        "# --- Preprocess the visualization sample *for the model* (normalization, tensor) ---\n",
        "inference_preprocess = A.Compose([\n",
        "    A.Normalize(mean=mean, std=std, max_pixel_value=255.0),\n",
        "    ToTensorV2()\n",
        "])\n",
        "processed = inference_preprocess(image=img_t1_vis_np, image1=img_t2_vis_np)\n",
        "img_t1_tensor = processed['image'].unsqueeze(0).to(DEVICE) # Add batch dim, send to device\n",
        "img_t2_tensor = processed['image1'].unsqueeze(0).to(DEVICE) # Add batch dim, send to device\n",
        "\n",
        "# --- Perform Inference ---\n",
        "print(\"Running inference...\")\n",
        "with torch.no_grad():\n",
        "    pred_logits = inference_model(img_t1_tensor, img_t2_tensor)\n",
        "    pred_probs = torch.sigmoid(pred_logits)\n",
        "    # Remove batch & channel dim, move to CPU, convert to numpy (H, W)\n",
        "    pred_mask_np = pred_probs.squeeze().cpu().numpy()\n",
        "print(\"Inference complete.\")\n",
        "\n",
        "# --- Post-process: Threshold and Find BBoxes ---\n",
        "binary_mask_np = (pred_mask_np > 0.5).astype(np.float32)\n",
        "bboxes = mask_to_bboxes(binary_mask_np, min_area_threshold=50) # Use threshold from function definition\n",
        "\n",
        "# --- Draw BBoxes on the T2 image used for visualization ---\n",
        "# This image is already IMG_SIZE x IMG_SIZE\n",
        "img_t2_with_boxes = draw_bboxes_on_image(img_t2_vis_np, bboxes, color=(0, 255, 0), thickness=2) # Green boxes\n",
        "\n",
        "# --- Display Results ---\n",
        "print(\"Displaying results...\")\n",
        "plt.figure(figsize=(25, 10)) # Wider figure\n",
        "\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.imshow(img_t1_vis_np)\n",
        "plt.title(f'Image T1 ({IMG_SIZE}x{IMG_SIZE})')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 5, 2)\n",
        "plt.imshow(img_t2_vis_np)\n",
        "plt.title(f'Image T2 ({IMG_SIZE}x{IMG_SIZE})')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 5, 3)\n",
        "plt.imshow(mask_vis_np, cmap='gray')\n",
        "plt.title('Ground Truth Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 5, 4)\n",
        "plt.imshow(binary_mask_np, cmap='gray')\n",
        "plt.title('Predicted Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 5, 5)\n",
        "plt.imshow(img_t2_with_boxes)\n",
        "plt.title(f'T2 w/ BBoxes ({len(bboxes)} detected)')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rYuw_4sfSpxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 11. Download the Trained Model\n",
        "from google.colab import files\n",
        "\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"\\nAttempting to download the best model: {MODEL_SAVE_PATH}\")\n",
        "    files.download(MODEL_SAVE_PATH)\n",
        "else:\n",
        "    print(f\"\\nModel file not found at {MODEL_SAVE_PATH}. Skipping download.\")\n",
        "    print(\"This might happen if training stopped early or failed.\")"
      ],
      "metadata": {
        "id": "n2N8L67PSssH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}