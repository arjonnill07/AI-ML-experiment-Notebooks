{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjonnill07/AI-ML-experiment-Notebooks/blob/main/12th_August_Best_Bangla_Topic_modeling_gradio_app_v1_4_30pm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk2Fvtit5JNp",
        "outputId": "7b921ca1-56f1-44a9-f302-0bc469b938ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "gradio\n",
        "pandas\n",
        "scikit-learn\n",
        "bertopic[visualization]\n",
        "sentence_transformers\n",
        "torch\n",
        "transformers\n",
        "accelerate\n",
        "bitsandbytes\n",
        "huggingface_hub\n",
        "requests\n",
        "# --- New dependencies for the scraper ---\n",
        "GoogleNews\n",
        "dateparser\n",
        "bnlp_toolkit\n",
        "bangla-stemmer\n",
        "nltk\n",
        "# --- New dependencies for the dashboard ---\n",
        "wordcloud\n",
        "matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This pip install command is for your Colab environment only.\n",
        "# It allows you to run and test the cells interactively.\n",
        "!pip install gradio pandas scikit-learn \"bertopic[visualization]\" sentence_transformers torch transformers accelerate bitsandbytes huggingface_hub requests -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn6_vNsu5e9u",
        "outputId": "bbb6db36-f01c-4287-c1dc-bd8186818443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: bertopic 0.17.3 does not provide the extra 'visualization'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile app.py\n",
        "# --- IMPORTS & GLOBAL SETUP ---\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import sqlite3\n",
        "import json\n",
        "import logging\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Transformers and BERTopic components\n",
        "from transformers import pipeline, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Hugging Face and Colab integration (optional, for LLM access)\n",
        "from huggingface_hub import login\n",
        "# from google.colab import userdata # We will disable this for HF Spaces deployment\n",
        "\n",
        "# Setup basic logging to monitor the application's health\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# A simple dictionary to hold data between UI interactions, acting as a session state.\n",
        "APP_STATE = {\n",
        "    \"df\": None,\n",
        "    \"bertopic_model\": None,\n",
        "    \"topics_df\": None,\n",
        "    \"final_df\": None,\n",
        "}\n",
        "\n",
        "print(\"✅ app.py created. Initial imports written.\")\n",
        "print(\"✅ Dependencies installed in Colab environment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNicuzNQ5W8f",
        "outputId": "8228079b-56bb-45c4-c700-f7b2dcac5592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "# --- TEXT PREPROCESSING & NORMALIZATION ---\n",
        "\n",
        "# A comprehensive list of Bangla stop words, tailored for news and general text.\n",
        "BANGLA_STOP_WORDS = [\n",
        "    'অতএব', 'অথচ', 'অথবা', 'অনুযায়ী', 'অনেক', 'অনেকে', 'অনেকেই', 'অন্তত', 'অন্য', 'অবধি', 'অবশ্য',\n",
        "    'অভিপ্রায়', 'একে', 'একই', 'একেবারে', 'একটি', 'একবার', 'এখন', 'এখনও', 'এখানে', 'এখানেই', 'এটি',\n",
        "    'এতটাই', 'এতদূর', 'এতটুকু', 'এক', 'এবং', 'এবার', 'এমন', 'এমনভাবে', 'এর', 'এরা', 'এঁরা', 'এঁদের',\n",
        "    'এই', 'এইভাবে', 'ও', 'ওঁরা', 'ওঁর', 'ওঁদের', 'ওকে', 'ওখানে', 'ওদের', 'ওর', 'কাছ', 'কাছে', 'কাজ',\n",
        "    'কারণ', 'কিছু', 'কিছুই', 'কিন্তু', 'কিভাবে', 'কেন', 'কোন', 'কোনও', 'কোনো', 'ক্ষেত্রে', 'খুব',\n",
        "    'গুলি', 'গিয়ে', 'চায়', 'ছাড়া', 'জন্য', 'জানা', 'ঠিক', 'তিনি', 'তিন', 'তিনিও', 'তাকে', 'তাঁকে',\n",
        "    'তার', 'তাঁর', 'তারা', 'তাঁরা', 'তাদের', 'তাঁদের', 'তাহলে', ' থাকলেও', 'থেকে', 'মধ্যেই', 'মধ্যে',\n",
        "    'द्वारा', 'নয়', 'না', 'নিজের', 'নিজে', 'নিয়ে', 'পারেন', 'পারা', 'পারে', 'পরে', 'পর্যন্ত', 'পুনরায়',\n",
        "    'ফলে', 'বজায়', 'বা', 'বাদে', 'বার', 'বিশেষ', 'বিভিন্ন', 'ব্যবহার', 'ব্যাপারে', 'ভাবে', 'ভাবেই', 'মাধ্যমে',\n",
        "    'মতো', 'মতোই', 'যখন', 'যদি', 'যদিও', 'যা', 'যাকে', 'যাওয়া', 'যায়', 'যে', 'যেখানে', 'যেতে', 'যেমন',\n",
        "    'যেহেতু', 'রহিছে', 'শিক্ষা', 'শুধু', 'সঙ্গে', 'সব', 'সমস্ত', 'সম্প্রতি', 'সহ', 'সাধারণ', 'সামনে', 'হতে',\n",
        "    'হতেই', 'হবে', 'হয়', 'হয়তো', 'হয়', 'হচ্ছে', 'হত', 'হলে', 'হলেও', 'হয়নি', 'হাজার', 'হোওয়া', 'আরও', 'আমরা',\n",
        "    'আমার', 'আমি', 'আর', 'আগে', 'আগেই', 'আছে', 'আজ', 'তাকে', 'তাতে', 'তাদের', 'তাহার', 'তাহাতে', 'তাহারই',\n",
        "    'তথা', 'তথাপি', 'সে', 'সেই', 'সেখান', 'সেখানে', 'থেকে', 'নাকি', 'নাগাদ', 'দু', 'দুটি', 'সুতরাং',\n",
        "    'সম্পর্কে', 'সঙ্গেও', 'সর্বাধিক', 'সর্বদা', 'সহ', 'হৈতে', 'হইবে', 'হইয়া', 'হৈল', 'জানিয়েছেন', 'প্রতিবেদক'\n",
        "]\n",
        "\n",
        "def normalize_bangla_manual(text):\n",
        "    \"\"\"A robust, self-contained function to normalize Bangla text.\"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    replacements = {\n",
        "        '[\\u09F7]': '\\u09B0', '[\\u09F2]': '\\u09B2', '[\\u09E4]': '\\u098B', '[\\u09E5]': '\\u09E1',\n",
        "        '[\\u09FA]': '\\u09B8\\u09CD\\u09AE', '[\\u09FB]': '\\u0995\\u09CD\\u09B7', '[\\u0970]': '\\u0966',\n",
        "        '[\\u09F3]': '\\u09B0\\u09C2', '[\\u09F8]': '\\u09A3', '[\\u09F9]': '\\u09B6', '[\\u0984]': '',\n",
        "        '[\\u0980]': '\\u0981', r'(\\s)।(\\s)': r'\\1।\\2', r'(\\S)।(\\S)': r'\\1 । \\2',\n",
        "        '[\\u0964][\\u0964]': '\\u0964', '[|]': '\\u0964', '[\\u09DC]': '\\u09A1\\u09BC',\n",
        "        '[\\u09DD]': '\\u09A2\\u09BC', '[\\u09DF]': '\\u09AF\\u09BC',\n",
        "    }\n",
        "    for old, new in replacements.items():\n",
        "        text = re.sub(old, new, text)\n",
        "    return text\n",
        "\n",
        "def preprocess_bangla_text(text):\n",
        "    \"\"\"Cleans and normalizes a single Bangla text string for NLP tasks.\"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = normalize_bangla_manual(text)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
        "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in BANGLA_STOP_WORDS]\n",
        "    text = \" \".join(words)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(\"✅ Helper functions appended to app.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h_ja8p56YDX",
        "outputId": "2c941db8-cf06-45b4-e90e-5cf5acb91516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "# --- APP BRANDING & CONFIGURATION ---\n",
        "# Easily update the application's title, tagline, and footer here.\n",
        "APP_TITLE = \"Social Perception Analyzer\"\n",
        "APP_TAGLINE = \"Prepared for the Policymakers of Bangladesh Nationalist Party (BNP)\"\n",
        "APP_FOOTER = \"Developed by Arjon and AI Studio\"\n",
        "\n",
        "\n",
        "# --- LOCAL LLM INITIALIZATION ---\n",
        "def initialize_local_llm(hf_token=None):\n",
        "    \"\"\"\n",
        "    Initializes and returns a local, quantized, lightweight LLM pipeline.\n",
        "    This model is chosen for its efficiency and Bangla language specialization.\n",
        "    \"\"\"\n",
        "    model_id = \"hishab/titulm-llama-3.2-1b-v1.1\"\n",
        "\n",
        "    # 4-bit quantization to reduce memory usage significantly\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Check for GPU availability\n",
        "        if not torch.cuda.is_available():\n",
        "            logging.warning(\"GPU not available. LLM will run on CPU and be very slow.\")\n",
        "            llm_pipeline = pipeline(\"text-generation\", model=model_id, token=hf_token)\n",
        "        else:\n",
        "            logging.info(f\"Initializing quantized local LLM: {model_id} on GPU.\")\n",
        "            llm_pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model_id,\n",
        "                model_kwargs={\"quantization_config\": quantization_config},\n",
        "                device_map=\"auto\",\n",
        "                token=hf_token\n",
        "            )\n",
        "        return llm_pipeline\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to initialize local LLM: {e}\")\n",
        "        # Add a note about potential trust issues for some models\n",
        "        logging.info(\"Trying again with 'trust_remote_code=True'.\")\n",
        "        try:\n",
        "             llm_pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model_id,\n",
        "                model_kwargs={\"trust_remote_code\": True, \"quantization_config\": quantization_config},\n",
        "                device_map=\"auto\",\n",
        "                token=hf_token\n",
        "            )\n",
        "             return llm_pipeline\n",
        "        except Exception as e2:\n",
        "             logging.error(f\"Secondary attempt failed: {e2}\")\n",
        "             gr.Warning(\"Could not initialize the local LLM. AI features will be disabled.\")\n",
        "             return None\n",
        "\n",
        "# --- DATA LOADING HELPER ---\n",
        "def load_data(file_obj, gsheet_url):\n",
        "    \"\"\"Loads a DataFrame from either an uploaded file or a Google Sheets URL.\"\"\"\n",
        "    if file_obj is not None:\n",
        "        logging.info(f\"Loading data from uploaded file: {file_obj.name}\")\n",
        "        return pd.read_csv(file_obj.name)\n",
        "    elif gsheet_url and gsheet_url.strip():\n",
        "        logging.info(f\"Loading data from Google Sheets URL.\")\n",
        "        try:\n",
        "            # Manipulate the URL for direct CSV export\n",
        "            csv_url = gsheet_url.replace('/edit?usp=sharing', '/export?format=csv&gid=0')\n",
        "            response = requests.get(csv_url)\n",
        "            response.raise_for_status() # Raise an exception for bad status codes\n",
        "            return pd.read_csv(StringIO(response.text))\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to load from Google Sheets URL. Please ensure the link is correct and publicly accessible. Error: {e}\")\n",
        "    else:\n",
        "        raise ValueError(\"Please upload a CSV file or provide a public Google Sheets URL.\")\n",
        "\n",
        "print(\"✅ App branding, LLM initialization, and data loading functions appended to app.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUUPd-J07tGn",
        "outputId": "2f37ab5e-04a7-4248-8448-f1d580cbf485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "# --- MAIN ANALYSIS ENGINE ---\n",
        "\n",
        "# We will define the AI agent in the next cell. For now, this is a placeholder.\n",
        "LLM_PIPELINE = None\n",
        "\n",
        "def run_analysis_pipeline(file_obj, gsheet_url, text_columns, analysis_mode, manual_seeds,\n",
        "                          top_n_topics_slider, enable_ai_merging, hf_token, progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    The main orchestrator function for the analysis pipeline.\n",
        "    This function incorporates all our agreed-upon refinements.\n",
        "    \"\"\"\n",
        "    global LLM_PIPELINE\n",
        "    if enable_ai_merging and LLM_PIPELINE is None:\n",
        "        progress(0, desc=\"Initializing LLM...\")\n",
        "        LLM_PIPELINE = initialize_local_llm(hf_token)\n",
        "        if LLM_PIPELINE is None:\n",
        "            gr.Warning(\"AI features enabled, but LLM failed to initialize. Skipping AI steps.\")\n",
        "            enable_ai_merging = False\n",
        "\n",
        "    # === STEP 1: LOAD AND VALIDATE DATA ===\n",
        "    progress(0.1, desc=\"Step 1/8: Loading and Validating Data...\")\n",
        "    try:\n",
        "        df = load_data(file_obj, gsheet_url)\n",
        "        if not text_columns: raise ValueError(\"Please select at least one text column to analyze.\")\n",
        "        df['combined_text'] = df[text_columns].fillna('').astype(str).agg(' '.join, axis=1)\n",
        "        df.dropna(subset=['combined_text'], inplace=True)\n",
        "        df['processed_text'] = df['combined_text'].apply(preprocess_bangla_text)\n",
        "\n",
        "        # REFINEMENT: Filter by word count for more robust document validation.\n",
        "        df_analysis = df[df['processed_text'].str.split().str.len() > 2].copy()\n",
        "        if df_analysis.empty:\n",
        "            raise ValueError(\"No documents with sufficient content found after cleaning. Please check your data and column selection.\")\n",
        "        documents = df_analysis['processed_text'].tolist()\n",
        "        APP_STATE[\"df\"] = df_analysis # Save the analyzable dataframe\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Data Loading Error: {e}\")\n",
        "        return {log_output: f\"Error during data loading: {e}\"}\n",
        "\n",
        "    # === STEP 2: PREPARE GUIDANCE (IF MANUAL SEEDING) ===\n",
        "    progress(0.2, desc=\"Step 2/8: Preparing Analysis Mode...\")\n",
        "    y_guidance = None\n",
        "    if analysis_mode == \"Manual Seeding\" and manual_seeds:\n",
        "        try:\n",
        "            seed_topics_dict = json.loads(manual_seeds)\n",
        "            y_guidance = [-1] * len(documents)\n",
        "            topic_name_to_id = {name: i for i, name in enumerate(seed_topics_dict.keys())}\n",
        "            for i, doc in enumerate(documents):\n",
        "                for topic_name, keywords in seed_topics_dict.items():\n",
        "                    if any(keyword in doc for keyword in keywords):\n",
        "                        y_guidance[i] = topic_name_to_id[topic_name]\n",
        "                        break # Prioritizes the first match in the JSON\n",
        "        except Exception as e:\n",
        "            return {log_output: f\"Error: Invalid JSON in Manual Seeds. Details: {e}\"}\n",
        "\n",
        "    # === STEP 3: EMBEDDINGS & MODEL SETUP (WITH REFINEMENTS) ===\n",
        "    progress(0.3, desc=\"Step 3/8: Calculating Document Embeddings...\")\n",
        "    embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "    embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
        "\n",
        "    # REFINEMENT: Lower min_cluster_size for more sensitive topic detection.\n",
        "    hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "    # REFINEMENT: Use max_df and min_df for adaptive stop word filtering.\n",
        "    vectorizer_model = CountVectorizer(tokenizer=lambda doc: doc.split(), ngram_range=(1, 3), max_df=0.90, min_df=5)\n",
        "\n",
        "    # Other components remain robust\n",
        "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "    representation_model = KeyBERTInspired()\n",
        "\n",
        "    # === STEP 4: TRAIN TOPIC MODEL ===\n",
        "    progress(0.5, desc=\"Step 4/8: Training BERTopic Model...\")\n",
        "    topic_model = BERTopic(\n",
        "        embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model, representation_model=representation_model,\n",
        "        language=\"multilingual\", verbose=False\n",
        "    )\n",
        "    topics, _ = topic_model.fit_transform(documents, embeddings, y=y_guidance)\n",
        "\n",
        "    # === STEP 5: AI REFINEMENT (IF ENABLED) ===\n",
        "    if enable_ai_merging and LLM_PIPELINE:\n",
        "        progress(0.6, desc=\"Step 5/8: Running AI Refinement Agent...\")\n",
        "        # We will define `run_ai_refinement` in the next cell. This is the hook.\n",
        "        topic_model = run_ai_refinement(topic_model, LLM_PIPELINE, progress)\n",
        "    else:\n",
        "        progress(0.6, desc=\"Step 5/8: Skipping AI Refinement...\")\n",
        "        # Fallback to default naming if AI is disabled\n",
        "        generated_labels = topic_model.generate_topic_labels(nr_words=4, separator=\", \")\n",
        "        topic_model.set_topic_labels(generated_labels)\n",
        "\n",
        "    # === STEP 6: APPLY MANUAL SEED NAMES ===\n",
        "    progress(0.7, desc=\"Step 6/8: Finalizing Topic Names...\")\n",
        "    if analysis_mode == \"Manual Seeding\" and 'seed_topics_dict' in locals():\n",
        "        for topic_name, topic_id in topic_name_to_id.items():\n",
        "            if topic_id in topic_model.get_topic_info()['Topic'].values:\n",
        "                topic_model.set_topic_labels({topic_id: topic_name})\n",
        "\n",
        "    # === STEP 7: PREPARE FINAL OUTPUTS & VISUALIZATIONS ===\n",
        "    progress(0.85, desc=\"Step 7/8: Preparing Visualizations...\")\n",
        "    APP_STATE[\"bertopic_model\"] = topic_model\n",
        "    df_analysis['Topic'] = topics\n",
        "    APP_STATE[\"final_df\"] = df_analysis\n",
        "    topics_df = topic_model.get_topic_info()\n",
        "    APP_STATE[\"topics_df\"] = topics_df\n",
        "\n",
        "    # REFINEMENT: Safeguard against memory errors on very large datasets.\n",
        "    if len(documents) > 50000:\n",
        "        gr.Info(\"Dataset is large. Visualizing a sample of 50,000 documents for performance.\")\n",
        "        indices = np.random.choice(len(documents), 50000, replace=False)\n",
        "        sampled_docs = [documents[i] for i in indices]\n",
        "        sampled_embeddings = embeddings[indices]\n",
        "        doc_topic_landscape_plot = topic_model.visualize_documents(sampled_docs, embeddings=sampled_embeddings)\n",
        "    else:\n",
        "        doc_topic_landscape_plot = topic_model.visualize_documents(documents, embeddings=embeddings)\n",
        "\n",
        "    inter_topic_map_plot = topic_model.visualize_topics()\n",
        "    # REFINEMENT: Use slider value for dynamic chart generation.\n",
        "    num_chart_topics = int(top_n_topics_slider)\n",
        "    top_topics_barchart_plot = topic_model.visualize_barchart(top_n_topics=num_chart_topics)\n",
        "    topic_similarity_heatmap_plot = topic_model.visualize_heatmap(top_n_topics=num_chart_topics)\n",
        "    topic_hierarchy_plot = topic_model.visualize_hierarchy(top_n_topics=num_chart_topics)\n",
        "\n",
        "    review_topic_table = topics_df[['Topic', 'Name', 'Count']].rename(columns={'Topic':'ID', 'Name':'Topic Name', 'Count':'Documents'})\n",
        "\n",
        "    # Check for date columns for the temporal analysis tab\n",
        "    date_columns = [col for col in df_analysis.columns if pd.to_datetime(df_analysis[col], errors='coerce').notna().any()]\n",
        "\n",
        "    # === STEP 8: UPDATE UI WITH RESULTS ===\n",
        "    progress(1.0, desc=\"Step 8/8: Finalizing UI...\")\n",
        "    return {\n",
        "        log_output: f\"✅ Analysis Complete! Discovered {len(topics_df)-1} topics.\",\n",
        "        # Make result tabs visible\n",
        "        review_tab: gr.update(visible=True),\n",
        "        visualize_tab: gr.update(visible=True),\n",
        "        # Populate the review tab\n",
        "        review_topic_table_df: gr.update(value=review_topic_table),\n",
        "        # Populate the visualization tab\n",
        "        doc_topic_landscape_plot_ui: doc_topic_landscape_plot,\n",
        "        inter_topic_map_plot_ui: inter_topic_map_plot, # Hook for the fixed plot\n",
        "        top_topics_barchart_plot_ui: top_topics_barchart_plot,\n",
        "        topic_similarity_heatmap_ui: topic_similarity_heatmap_plot,\n",
        "        topic_hierarchy_plot_ui: topic_hierarchy_plot,\n",
        "        # Update and enable the temporal analysis tab if date columns exist\n",
        "        temporal_analysis_group: gr.update(visible=len(date_columns) > 0),\n",
        "        date_column_dropdown: gr.update(choices=date_columns, value=date_columns[0] if date_columns else None),\n",
        "    }\n",
        "\n",
        "print(\"✅ Main analysis pipeline function appended to app.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq3fwlQX7wGt",
        "outputId": "8c5c5aad-c963-4159-f626-2078c7e15245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "# --- AI REFINEMENT AGENT ---\n",
        "\n",
        "def run_ai_refinement(topic_model, llm_pipeline, progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    Uses a lightweight LLM to generate high-quality, contextual topic names.\n",
        "    Includes a conceptual hook for future AI-powered topic merging.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting AI Refinement Agent...\")\n",
        "\n",
        "    # --- Task 1: AI-Powered Topic Naming ---\n",
        "    progress(0, desc=\"AI Agent: Generating Topic Names...\")\n",
        "    topic_info_df = topic_model.get_topic_info()\n",
        "    new_labels = {}\n",
        "\n",
        "    # This is the advanced, few-shot Bangla prompt we designed.\n",
        "    # It will be used for each topic.\n",
        "    prompt_template = \"\"\"\n",
        "আপনি একজন পেশাদার সংবাদ সম্পাদক। আপনার কাজ হলো বাংলাদেশের রাজনৈতিক ঘটনাবলী, বিশেষ করে বিএনপির 'তারুণ্যের সমাবেশ' সংক্রান্ত সংবাদের জন্য একটি সংক্ষিপ্ত ও প্রাসঙ্গিক শিরোনাম তৈরি করা। প্রদত্ত কীওয়ার্ডগুলো ব্যবহার করে একটি (৩-৫ শব্দের) সারগর্ভ বাংলা শিরোনাম লিখুন, যেখানে সমাবেশের মূল বিষয় বা স্থান স্পষ্টভাবে ফুটে উঠবে। উদাহরণগুলো দেখুন।\n",
        "\n",
        "--- উদাহরণ ---\n",
        "ইনপুট কীওয়ার্ড: ['খুলনা', 'তারুণ্যের', 'সমাবেশ', 'বিএনপি']\n",
        "আউটপুট শিরোনাম: খুলনায় বিএনপির তারুণ্যের সমাবেশ\n",
        "\n",
        "ইনপুট কীওয়ার্ড: ['ঢাকা', 'নয়াপল্টন', 'তারুণ্যের', 'স্রোত', 'বৃষ্টি']\n",
        "আউটপুট শিরোনাম: ঢাকায় তারুণ্যের সমাবেশে জনতার ঢল\n",
        "\n",
        "ইনপুট কীওয়ার্ড: ['চট্টগ্রাম', 'বক্তব্য', 'মির্জা ফখরুল', 'শোডাউন']\n",
        "আউটপুট শিরোনাম: চট্টগ্রামে মির্জা ফখরুলের তারুণ্যের সমাবেশ\n",
        "--- উদাহরণের শেষ ---\n",
        "\n",
        "--- আপনার কাজ ---\n",
        "ইনপুট কীওয়ার্ড: {keywords}\n",
        "আউটপুট শিরোনাম:\n",
        "\"\"\"\n",
        "\n",
        "    # Tuned parameters for reliable, non-creative naming\n",
        "    generation_params = {\n",
        "        \"temperature\": 0.3,\n",
        "        \"max_new_tokens\": 30,\n",
        "        \"repetition_penalty\": 1.2,\n",
        "        \"do_sample\": True\n",
        "    }\n",
        "\n",
        "    # Iterate through each topic to generate a new name\n",
        "    for index, row in topic_info_df.iterrows():\n",
        "        topic_id = row['Topic']\n",
        "        if topic_id == -1:\n",
        "            # We don't rename the outlier topic\n",
        "            new_labels[topic_id] = \"Topic -1: Outliers\"\n",
        "            continue\n",
        "\n",
        "        keywords = row['Representation']\n",
        "\n",
        "        # Format the prompt for the current topic\n",
        "        prompt = prompt_template.format(keywords=keywords)\n",
        "\n",
        "        try:\n",
        "            # Call the LLM pipeline\n",
        "            response = llm_pipeline(prompt, **generation_params)\n",
        "            # Extract the generated text, stripping whitespace and the prompt's artifacts\n",
        "            generated_name = response[0]['generated_text'].split(\"আউটপুট শিরোনাম:\")[1].strip()\n",
        "\n",
        "            if generated_name:\n",
        "                new_labels[topic_id] = f\"Topic {topic_id}: {generated_name}\"\n",
        "                logging.info(f\"Generated name for Topic {topic_id}: {generated_name}\")\n",
        "            else:\n",
        "                # Fallback to default name if generation fails\n",
        "                new_labels[topic_id] = topic_model.get_topic_label(topic_id, nr_words=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"LLM failed for Topic {topic_id}. Error: {e}\")\n",
        "            # Fallback for safety\n",
        "            new_labels[topic_id] = topic_model.get_topic_label(topic_id, nr_words=4)\n",
        "\n",
        "        progress.update((index + 1) / len(topic_info_df))\n",
        "\n",
        "    # Apply all the new, AI-generated labels at once\n",
        "    topic_model.set_topic_labels(new_labels)\n",
        "    logging.info(\"✅ AI Naming complete.\")\n",
        "\n",
        "    # --- Task 2: AI-Powered Merging (Conceptual Hook) ---\n",
        "    # This section is a placeholder for a future enhancement.\n",
        "    # The logic would be:\n",
        "    # 1. Calculate topic similarity matrix.\n",
        "    # 2. Identify pairs with similarity > threshold (e.g., 0.85).\n",
        "    # 3. Use a \"Judge\" prompt to ask the LLM if they should be merged.\n",
        "    # 4. If LLM says \"YES\", call `topic_model.merge_topics()`.\n",
        "    logging.info(\"Skipping AI Topic Merging (conceptual feature).\")\n",
        "\n",
        "    return topic_model\n",
        "\n",
        "print(\"✅ AI Refinement Agent function appended to app.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq426SxG8h7o",
        "outputId": "5ca1226b-5099-4a81-f982-a1c20a70d369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "# --- FINAL BACKEND HANDLERS & HELPERS ---\n",
        "\n",
        "def get_topic_details(topic_id: int):\n",
        "    \"\"\"Fetches details for a selected topic to display in the review tab.\"\"\"\n",
        "    empty_return = {topic_name_textbox: \"\", topic_word_cloud_plot: None, topic_docs_df: pd.DataFrame()}\n",
        "    model = APP_STATE.get(\"bertopic_model\")\n",
        "    if model is None or topic_id is None: return empty_return\n",
        "    try:\n",
        "        topic_id = int(topic_id)\n",
        "        topic_info = model.get_topic_info(topic_id=topic_id)\n",
        "        if topic_info.empty: return empty_return\n",
        "\n",
        "        # Strip the \"Topic X: \" prefix for cleaner editing\n",
        "        topic_name = topic_info['Name'].iloc[0]\n",
        "        cleaned_name = re.sub(r'^Topic \\d+:\\s*', '', topic_name)\n",
        "\n",
        "        # For the outlier topic, don't generate plots\n",
        "        if topic_id == -1:\n",
        "            return {topic_name_textbox: cleaned_name, topic_word_cloud_plot: None, topic_docs_df: pd.DataFrame()}\n",
        "\n",
        "        word_cloud_fig = model.visualize_barchart(top_n_topics=1, topics=[topic_id])\n",
        "        docs_df = pd.DataFrame(model.get_representative_docs(topic_id), columns=['Representative Document'])\n",
        "        return {topic_name_textbox: cleaned_name, topic_word_cloud_plot: word_cloud_fig, topic_docs_df: docs_df}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error getting topic details for ID {topic_id}: {e}\")\n",
        "        return empty_return\n",
        "\n",
        "def update_topic_name(topic_id, new_name):\n",
        "    \"\"\"Handler for manual topic renaming.\"\"\"\n",
        "    model = APP_STATE.get(\"bertopic_model\")\n",
        "    if model and topic_id is not None and new_name:\n",
        "        topic_id = int(topic_id)\n",
        "        # Add the prefix back for consistency\n",
        "        full_name = f\"Topic {topic_id}: {new_name}\"\n",
        "        model.set_topic_labels({topic_id: full_name})\n",
        "        APP_STATE[\"topics_df\"] = model.get_topic_info()\n",
        "        gr.Info(f\"Topic {topic_id} renamed to '{new_name}'\")\n",
        "        # Return the updated table for the UI\n",
        "        return gr.update(value=APP_STATE[\"topics_df\"][['Topic', 'Name', 'Count']].rename(columns={'Topic':'ID', 'Name':'Topic Name', 'Count':'Documents'}))\n",
        "    return gr.update() # No change\n",
        "\n",
        "def merge_selected_topics(topics_to_merge):\n",
        "    \"\"\"Handler for manual topic merging.\"\"\"\n",
        "    model = APP_STATE.get(\"bertopic_model\")\n",
        "    if model and topics_to_merge and len(topics_to_merge) > 1:\n",
        "        # Convert topic names like \"Topic 0: ...\" to integer IDs\n",
        "        topic_ids = [int(re.search(r'\\d+', t).group()) for t in topics_to_merge]\n",
        "\n",
        "        model.merge_topics(topics_to_merge=[topic_ids])\n",
        "\n",
        "        # After merging, we need to refresh the state and UI components\n",
        "        APP_STATE[\"topics_df\"] = model.get_topic_info()\n",
        "        review_topic_table = APP_STATE[\"topics_df\"][['Topic', 'Name', 'Count']].rename(columns={'Topic':'ID', 'Name':'Topic Name', 'Count':'Documents'})\n",
        "\n",
        "        gr.Info(f\"Successfully merged topics: {topic_ids}\")\n",
        "        return {\n",
        "            review_topic_table_df: gr.update(value=review_topic_table),\n",
        "            # Clear the selection and the details view\n",
        "            topic_merger_checkboxgroup: gr.update(value=[]),\n",
        "            topic_name_textbox: \"\",\n",
        "            topic_word_cloud_plot: None,\n",
        "            topic_docs_df: pd.DataFrame(),\n",
        "        }\n",
        "    gr.Warning(\"Please select at least two topics to merge.\")\n",
        "    return {review_topic_table_df: gr.update(), topic_merger_checkboxgroup: gr.update()}\n",
        "\n",
        "\n",
        "def generate_temporal_plot(date_column, progress=gr.Progress()):\n",
        "    \"\"\"Generates and displays the topics over time plot.\"\"\"\n",
        "    progress(0, desc=\"Preparing time data...\")\n",
        "    if not date_column: return None\n",
        "    model, df = APP_STATE.get(\"bertopic_model\"), APP_STATE.get(\"final_df\")\n",
        "    if model is None or df is None: return None\n",
        "\n",
        "    df_temporal = df.copy()\n",
        "    df_temporal['timestamp'] = pd.to_datetime(df_temporal[date_column], errors='coerce')\n",
        "    df_temporal.dropna(subset=['timestamp'], inplace=True)\n",
        "\n",
        "    if df_temporal.empty:\n",
        "        gr.Warning(f\"The column '{date_column}' contains no valid dates after conversion.\")\n",
        "        return None\n",
        "\n",
        "    progress(0.6, desc=\"Generating topic trends over time...\")\n",
        "    try:\n",
        "        # BERTopic requires the original documents and timestamps for this plot\n",
        "        docs_temporal = df_temporal['processed_text'].tolist()\n",
        "        timestamps_temporal = df_temporal['timestamp'].tolist()\n",
        "        topics_over_time = model.topics_over_time(docs=docs_temporal, timestamps=timestamps_temporal)\n",
        "        return model.visualize_topics_over_time(topics_over_time)\n",
        "    except Exception as e:\n",
        "        gr.Error(f\"Could not generate temporal plot. This can happen if topics are not found in the selected time range. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_media_analysis(media_column):\n",
        "    \"\"\"Generates a bar chart for media source analysis.\"\"\"\n",
        "    if not media_column:\n",
        "        gr.Warning(\"Please select a media column to analyze.\")\n",
        "        return None\n",
        "    df = APP_STATE.get(\"df\")\n",
        "    if df is None or media_column not in df.columns:\n",
        "        return None\n",
        "\n",
        "    counts = df[media_column].value_counts().nlargest(20) # Get top 20 sources\n",
        "\n",
        "    # Using Gradio's built-in plotting for simplicity\n",
        "    plot_df = pd.DataFrame({'Media Source': counts.index, 'Article Count': counts.values})\n",
        "    return gr.BarPlot(\n",
        "        plot_df,\n",
        "        x='Media Source',\n",
        "        y='Article Count',\n",
        "        title=f'Top 20 Media Sources by Article Count',\n",
        "        tooltip=['Media Source', 'Article Count'],\n",
        "        height=500,\n",
        "        vertical_guides=[{'value': counts.mean(), 'label': 'Average'}]\n",
        "    )\n",
        "\n",
        "def finalize_and_save():\n",
        "    \"\"\"Saves the final DataFrame and topic definitions to files.\"\"\"\n",
        "    if APP_STATE.get(\"final_df\") is None or APP_STATE.get(\"topics_df\") is None:\n",
        "        gr.Warning(\"No data available to save.\")\n",
        "        return None\n",
        "\n",
        "    final_df_to_save, topics_df_to_save = APP_STATE[\"final_df\"].copy(), APP_STATE[\"topics_df\"].copy()\n",
        "\n",
        "    # Convert list columns to JSON strings for compatibility\n",
        "    for col in ['Representation', 'Representative_Docs']:\n",
        "        if col in topics_df_to_save.columns:\n",
        "            topics_df_to_save[col] = topics_df_to_save[col].apply(\n",
        "                lambda x: json.dumps(x) if isinstance(x, list) else x\n",
        "            )\n",
        "\n",
        "    db_path, csv_path = \"topic_analysis_results.sqlite\", \"labeled_documents.csv\"\n",
        "\n",
        "    with sqlite3.connect(db_path) as conn:\n",
        "        topics_df_to_save.to_sql(\"topic_definitions\", conn, if_exists=\"replace\", index=False)\n",
        "        final_df_to_save.to_sql(\"enriched_documents\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    topic_map = topics_df_to_save.set_index('Topic')['Name'].to_dict()\n",
        "    final_df_to_save['Topic_Name'] = final_df_to_save['Topic'].map(topic_map)\n",
        "    final_df_to_save.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    gr.Info(f\"Results saved to {db_path} and {csv_path}\")\n",
        "    return [db_path, csv_path]\n",
        "\n",
        "print(\"✅ Final backend handlers appended to app.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cacr0bicBDBH",
        "outputId": "f35b463e-03f2-465b-c53e-91b22fb9330b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "# --- GRADIO UI LAYOUT & EVENT HANDLERS ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=APP_TITLE) as app:\n",
        "    gr.Markdown(f\"# {APP_TITLE}\")\n",
        "    gr.Markdown(f\"*{APP_TAGLINE}*\")\n",
        "\n",
        "    with gr.Tabs() as tabs:\n",
        "        # === SETUP & RUN TAB ===\n",
        "        with gr.TabItem(\"1. Setup & Run Analysis\", id=0):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### 1. Data Input\")\n",
        "                    file_upload = gr.File(label=\"Upload CSV File\", file_types=[\".csv\"])\n",
        "                    gsheet_url = gr.Textbox(label=\"Or Paste Google Sheets URL\", placeholder=\"https://docs.google.com/spreadsheets/d/...\")\n",
        "\n",
        "                    gr.Markdown(\"### 2. Select Columns\")\n",
        "                    text_columns_checkboxgroup = gr.CheckboxGroup(label=\"Select Text Columns for Analysis\", interactive=True)\n",
        "\n",
        "                    gr.Markdown(\"### 3. Configure Analysis\")\n",
        "                    analysis_mode_radio = gr.Radio([\"Discovery Mode\", \"Manual Seeding\"], value=\"Discovery Mode\", label=\"Analysis Mode\")\n",
        "                    manual_seeds_textbox = gr.Textbox(label=\"Manual Seed Topics (JSON format)\", visible=False, lines=5)\n",
        "                    # FIX: Assign the markdown to a variable so we can target it directly\n",
        "                    manual_seeds_example = gr.Markdown(\"Example: `{\\\"Topic A\\\": [\\\"keyword1\\\", \\\"keyword2\\\"], \\\"Topic B\\\": [\\\"wordA\\\", \\\"wordB\\\"]}`\", visible=False)\n",
        "\n",
        "                    top_n_topics_slider = gr.Slider(label=\"Number of Topics for Charts\", minimum=5, maximum=50, value=15, step=1)\n",
        "\n",
        "                    gr.Markdown(\"### 4. Advanced (Optional)\")\n",
        "                    enable_ai_merging_checkbox = gr.Checkbox(label=\"Enable AI Topic Naming (Requires GPU & HF Token)\", value=False)\n",
        "                    hf_token_textbox = gr.Textbox(label=\"Hugging Face Token\", type=\"password\", placeholder=\"hf_...\", info=\"Required if AI is enabled.\")\n",
        "\n",
        "                    start_button = gr.Button(\"Start Analysis\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    log_output = gr.Textbox(label=\"Pipeline Progress\", lines=25, interactive=False, autoscroll=True)\n",
        "\n",
        "        # === REVIEW & FINALIZE TAB ===\n",
        "        with gr.TabItem(\"2. Review & Finalize\", id=1, visible=False) as review_tab:\n",
        "            gr.Markdown(\"### Review, Refine, and Finalize Your Topic Model\")\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Markdown(\"**Topics Found**\")\n",
        "                    review_topic_table_df = gr.DataFrame(headers=[\"ID\", \"Topic Name\", \"Documents\"], interactive=True, wrap=True, scale=2)\n",
        "                with gr.Column(scale=3):\n",
        "                    gr.Markdown(\"**Selected Topic Details**\")\n",
        "                    topic_id_state = gr.State() # Hidden state to store the selected topic ID\n",
        "                    topic_name_textbox = gr.Textbox(label=\"Topic Name (Editable)\")\n",
        "                    update_name_button = gr.Button(\"Update Name\")\n",
        "                    topic_word_cloud_plot = gr.Plot(label=\"Top Words for Selected Topic\")\n",
        "                    topic_docs_df = gr.DataFrame(headers=[\"Representative Document\"], wrap=True)\n",
        "\n",
        "            with gr.Row():\n",
        "                gr.Markdown(\"### Manual Topic Merging\")\n",
        "            with gr.Row():\n",
        "                topic_merger_checkboxgroup = gr.CheckboxGroup(label=\"Select 2 or more topics to merge\", interactive=True)\n",
        "                merge_button = gr.Button(\"Merge Selected Topics\", variant=\"stop\")\n",
        "            with gr.Row():\n",
        "                finalize_button = gr.Button(\"Save Final Results to Files\", variant=\"primary\")\n",
        "                download_link = gr.File(label=\"Download Results (SQLite DB and CSV)\", file_count=\"multiple\")\n",
        "\n",
        "\n",
        "        # === VISUALIZE & EXPLORE TAB ===\n",
        "        with gr.TabItem(\"3. Visualize & Explore\", id=2, visible=False) as visualize_tab:\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"Document Landscape\"):\n",
        "                    gr.Markdown(\"A 2D map of every document, colored by its assigned topic. This shows the overall structure of your data.\")\n",
        "                    doc_topic_landscape_plot_ui = gr.Plot()\n",
        "                with gr.TabItem(\"Topic Relationships\"):\n",
        "                    gr.Markdown(\"Visualizations showing how topics relate to each other.\")\n",
        "                    inter_topic_map_plot_ui = gr.Plot(label=\"Inter-Topic Distance Map\")\n",
        "                    topic_hierarchy_plot_ui = gr.Plot(label=\"Hierarchical Clustering of Topics\")\n",
        "                    topic_similarity_heatmap_ui = gr.Plot(label=\"Topic Similarity Heatmap\")\n",
        "                with gr.TabItem(\"Topic Keywords\"):\n",
        "                    gr.Markdown(\"A bar chart showing the most important keywords for the most prominent topics.\")\n",
        "                    top_topics_barchart_plot_ui = gr.Plot()\n",
        "                with gr.TabItem(\"Temporal Analysis\"):\n",
        "                    with gr.Group(visible=False) as temporal_analysis_group:\n",
        "                        gr.Markdown(\"Select a date column from your data to see how topic popularity has changed over time.\")\n",
        "                        with gr.Row():\n",
        "                            date_column_dropdown = gr.Dropdown(label=\"Select Date Column\")\n",
        "                            generate_trends_button = gr.Button(\"Generate Trend Plot\")\n",
        "                        temporal_plot_ui = gr.Plot()\n",
        "\n",
        "        # === SOURCE ANALYSIS TAB ===\n",
        "        with gr.TabItem(\"4. Source Analysis\", id=3, visible=False) as source_tab:\n",
        "            gr.Markdown(\"### Analyze the Distribution of News Sources\")\n",
        "            with gr.Row():\n",
        "                media_column_dropdown = gr.Dropdown(label=\"Select Your Media/Source Column\")\n",
        "                analyze_media_button = gr.Button(\"Analyze Sources\")\n",
        "            with gr.Row():\n",
        "                media_plot = gr.BarPlot()\n",
        "\n",
        "    gr.Markdown(f\"<div style='text-align: center;'>{APP_FOOTER}</div>\")\n",
        "\n",
        "    # --- EVENT HANDLERS ---\n",
        "\n",
        "    def update_column_selector(file, url):\n",
        "        \"\"\"Populates column selectors after data is loaded.\"\"\"\n",
        "        # This function also makes the source analysis tab visible if data loads\n",
        "        if file is None and not url:\n",
        "            return {text_columns_checkboxgroup: gr.update(choices=[], value=None), media_column_dropdown: gr.update(choices=[], value=None), source_tab: gr.update(visible=False)}\n",
        "        try:\n",
        "            df = load_data(file, url)\n",
        "            text_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
        "            return {\n",
        "                text_columns_checkboxgroup: gr.update(choices=text_cols, value=text_cols if text_cols else None),\n",
        "                media_column_dropdown: gr.update(choices=df.columns.tolist()),\n",
        "                source_tab: gr.update(visible=True)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            gr.Warning(f\"Failed to read columns: {e}\")\n",
        "            return {text_columns_checkboxgroup: gr.update(choices=[], value=None), media_column_dropdown: gr.update(choices=[], value=None), source_tab: gr.update(visible=False)}\n",
        "\n",
        "    file_upload.upload(fn=update_column_selector, inputs=[file_upload, gsheet_url], outputs=[text_columns_checkboxgroup, media_column_dropdown, source_tab])\n",
        "    gsheet_url.submit(fn=update_column_selector, inputs=[file_upload, gsheet_url], outputs=[text_columns_checkboxgroup, media_column_dropdown, source_tab])\n",
        "\n",
        "    # FIX: A single, robust function to control the visibility of manual seeding UI elements\n",
        "    def toggle_manual_seeding_ui(mode):\n",
        "        is_visible = mode == \"Manual Seeding\"\n",
        "        return {\n",
        "            manual_seeds_textbox: gr.update(visible=is_visible),\n",
        "            manual_seeds_example: gr.update(visible=is_visible)\n",
        "        }\n",
        "\n",
        "    analysis_mode_radio.change(\n",
        "        fn=toggle_manual_seeding_ui,\n",
        "        inputs=analysis_mode_radio,\n",
        "        outputs=[manual_seeds_textbox, manual_seeds_example]\n",
        "    )\n",
        "\n",
        "    start_button.click(\n",
        "        fn=run_analysis_pipeline,\n",
        "        inputs=[file_upload, gsheet_url, text_columns_checkboxgroup, analysis_mode_radio, manual_seeds_textbox, top_n_topics_slider, enable_ai_merging_checkbox, hf_token_textbox],\n",
        "        outputs=[log_output, review_tab, visualize_tab, review_topic_table_df, doc_topic_landscape_plot_ui, inter_topic_map_plot_ui,\n",
        "                 top_topics_barchart_plot_ui, topic_similarity_heatmap_ui, topic_hierarchy_plot_ui, temporal_analysis_group, date_column_dropdown]\n",
        "    )\n",
        "\n",
        "    def on_select_topic(evt: gr.SelectData):\n",
        "        \"\"\"Handles selecting a topic from the main review table.\"\"\"\n",
        "        if not isinstance(evt.index, tuple) or len(evt.index) == 0:\n",
        "            return {topic_id_state: None, topic_name_textbox: \"\", topic_word_cloud_plot: None, topic_docs_df: pd.DataFrame()}\n",
        "        try:\n",
        "            topic_id_val = APP_STATE[\"topics_df\"].iloc[evt.index[0]]['ID']\n",
        "            details = get_topic_details(topic_id_val)\n",
        "            details[topic_id_state] = topic_id_val # Store the ID in the hidden state\n",
        "            return details\n",
        "        except Exception:\n",
        "            return {topic_id_state: None, topic_name_textbox: \"\", topic_word_cloud_plot: None, topic_docs_df: pd.DataFrame()}\n",
        "\n",
        "    review_topic_table_df.select(fn=on_select_topic, outputs=[topic_id_state, topic_name_textbox, topic_word_cloud_plot, topic_docs_df])\n",
        "\n",
        "    # Connect the new manual refinement buttons\n",
        "    update_name_button.click(fn=update_topic_name, inputs=[topic_id_state, topic_name_textbox], outputs=[review_topic_table_df])\n",
        "\n",
        "    # When the main results are generated, populate the topic merger checklist\n",
        "    review_topic_table_df.change(lambda df: gr.update(choices=df['Topic Name'].tolist()), inputs=review_topic_table_df, outputs=topic_merger_checkboxgroup)\n",
        "\n",
        "    merge_button.click(fn=merge_selected_topics, inputs=[topic_merger_checkboxgroup], outputs=[review_topic_table_df, topic_merger_checkboxgroup, topic_name_textbox, topic_word_cloud_plot, topic_docs_df])\n",
        "\n",
        "    # Connect the new Source Analysis tab\n",
        "    analyze_media_button.click(fn=generate_media_analysis, inputs=[media_column_dropdown], outputs=[media_plot])\n",
        "\n",
        "    # Other handlers\n",
        "    generate_trends_button.click(fn=generate_temporal_plot, inputs=[date_column_dropdown], outputs=[temporal_plot_ui])\n",
        "    finalize_button.click(fn=finalize_and_save, inputs=[], outputs=[download_link])\n",
        "\n",
        "# --- LAUNCH THE APP ---\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdFY4n97BFhF",
        "outputId": "0807f04e-357c-48ad-fe2d-5902b55ffad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZVKCH1lB83-",
        "outputId": "4ecc73b2-9a93-4333-8c63-28f78e87731a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-11 21:41:35.417492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754948495.694693    1278 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754948495.770746    1278 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754948496.324810    1278 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754948496.324861    1278 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754948496.324866    1278 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754948496.324871    1278 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-11 21:41:36.376075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✅ app.py created. Initial imports written.\n",
            "✅ Dependencies installed in Colab environment.\n",
            "✅ Helper functions appended to app.py\n",
            "✅ App branding, LLM initialization, and data loading functions appended to app.py\n",
            "✅ Main analysis pipeline function appended to app.py\n",
            "✅ AI Refinement Agent function appended to app.py\n",
            "✅ Final backend handlers appended to app.py\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://dac7dbf739cb7bdaf2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "WARNING:root:GPU not available. LLM will run on CPU and be very slow.\n",
            "config.json: 100% 885/885 [00:00<00:00, 4.07MB/s]\n",
            "model.safetensors: 100% 2.47G/2.47G [01:46<00:00, 23.1MB/s]\n",
            "generation_config.json: 100% 180/180 [00:00<00:00, 1.18MB/s]\n",
            "tokenizer_config.json: 51.1kB [00:00, 34.1MB/s]\n",
            "tokenizer.json: 9.09MB [00:00, 17.5MB/s]\n",
            "special_tokens_map.json: 100% 335/335 [00:00<00:00, 1.88MB/s]\n",
            "Device set to use cpu\n",
            "ERROR:root:Data Loading Error: No documents with sufficient content found after cleaning. Please check your data and column selection.\n",
            "ERROR:root:Data Loading Error: No documents with sufficient content found after cleaning. Please check your data and column selection.\n",
            "modules.json: 100% 229/229 [00:00<00:00, 1.57MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 810kB/s]\n",
            "README.md: 3.90kB [00:00, 15.6MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 389kB/s]\n",
            "config.json: 100% 723/723 [00:00<00:00, 4.11MB/s]\n",
            "model.safetensors: 100% 1.11G/1.11G [00:16<00:00, 67.3MB/s]\n",
            "tokenizer_config.json: 100% 402/402 [00:00<00:00, 2.72MB/s]\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:01<00:00, 3.82MB/s]\n",
            "tokenizer.json: 9.08MB [00:00, 83.6MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.23MB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.28MB/s]\n",
            "Batches: 100% 43/43 [02:22<00:00,  3.32s/it]\n"
          ]
        }
      ]
    }
  ]
}